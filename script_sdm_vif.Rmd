---
title: "Species Distribution Modeling in R"
author: "Luíz Fernando Esser, Reginaldo Ré, Edivando V. do Couto"
params:
  especie: "Colossoma.macropomum"
output:
  html_document:
    df_print: paged
    number_sections: TRUE    # seções numeradas automaticamente
    toc: true                # construir o sumário
    toc_float: true          # colocar o sumário flutuante no canto superior esquerdo
    toc_depth: 3             # o sumário conterá três níveis de profundidade: capitulo, seção e subseção
    theme: cerulean          # tema de cores estilo de fonte, use o "united" para um tema monocromico
    fig_width: 8             # largura padrão das figuras
editor_options: 
  chunk_output_type: inline
---

<style type="text/css">
.main-container {
  max-width: 2000px;
  margin-left: auto;
  margin-right: auto;
}
</style>


# Preparing the environment for script execution

1. Download maxent.jar and place it in the same folder as this document
     * Download from (http://biodiversityinformatics.amnh.org/open_source/maxent/./maxent.php?op=download)
     * After the first time this script runs in its entirety, this file can be deleted if you wish
2. Download and install R
3. Download and install RTools 3.5
     * Download from (https://cran.r-project.org/bin/windows/Rtools/Rtools35.exe)
     * When installing, check the option to put RTools in *path*
4. Download and install RStudio
     * Download from (https://download1.rstudio.org/desktop/windows/RStudio-1.2.5001.exe)
5. Install and load the devtools package (run the commands using the RStudio console)
     * `install.packages("devtools")`
     * `library(devtools)`
6. Install the sdm package and the packages it depends on (run the commands using the RStudio console)
     * `install.packages("sdm")`
     * `library(sdm)`
     * `installAll()`
7. Install all other required packages (run commands using RStudio console)
     * `install.packages(c("ade4","boot","cowplot","DT","factoextra","FactoMineR","fs","ggcorrplot","ggfortify","ggplot2","here","httr","janitor","lubridate","magrittr","mapview","parallel","paran","patchwork","plotly","purrrlyr","raster","rasterDT","rdist","rgbif","Rtsne","scales","sdm","sf","snakecase","snow","stars","stringr","tidyverse","usdm","vroom"))`
     

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r message=FALSE, warning=FALSE, include=FALSE}
source("config/script_funcoes_auxiliares.R")
```


# Downloading data:
## Downloading rasters
First we create a folder in our working directory to include input data.
```{r}
# Create folder to store inputs.
if(!dir_exists('input_data')){
  dir_create('input_data')
}
```


Now let's download data from WorldClim 2.1. Note that when R downloads a file it has a timeout of 60 seconds. This may not be enough to download environmental data, so we can set options(timeout=n), where n is the number of seconds we need to download the data.

```{r download_rasters_wc2.1, eval=T, include=T}
# This option allows us to control how much time do we need to download the data. If R takes more than 10 minutes (600 seconds) to download the data it will stop the download. Increase the timeout if needed.
options(timeout=600)

# Current data
# Files are automatically saved in input_data folder.
WorldClim_data('current', variable = 'bioc', resolution = 10)

# Future data
gcms <- c('cc', 'gg', 'mr', 'uk')
WorldClim_data('future', variable = 'bioc', year = '2090', gcm = gcms, ssp = c('245'), resolution = 10) 
```


## Obtaining occurrence data from GBIF:

```{r download_GBIF_data, eval=T, include=T}
# Downloading data from GBIF
# File is automatically saved in input_data folder
# spp_data <- GBIF_data('Colossoma macropomum')
```


## Downloading shapefile for study area

```{r}
# Obtaining Natural Earth data:
#shape_study_area <- rnaturalearth::ne_download(scale = 50, type = "rivers_lake_centerlines", category = "physical")
```

# Geoprocessing:
## Open Files and Data
Firstly, we name inputs and outputs, caring for using the correct extensions.
a) Inputs:

```{r}
# Shapefile (polygon or lines) delimiting study area.
shape_study_area_file <- here("input_data/shape_study_area/AmazonHydroRivers4.shp")  

# Directory name containing current rasters to be rescaled.
folder_current_rasters <- here("input_data/WorldClim_data_current")

# Directory name containing future rasters to be rescaled.
folder_future_rasters <- here("input_data/WorldClim_data_future")
```


b) Outputs:

```{r}
# Name of shapefile (.shp) for the study area to be saved.
output_shp_study_area <- here("output_data/grid/Amazon_grid.shp")

# Name of R object (.rds) where the current rescaled variables will be saved.
output_shp_current <- here("output_data/WorldClim_data_current_rescaled/Amazon_current.rds")

# Set scenarios names:
scenarios <- apply(expand.grid(gcms, c("ssp245"),"10", 2090), 1, paste, collapse="_")

# Name of R object (.rds) where the future rescaled variables will be saved.
output_shp_future <- here(paste0("output_data/WorldClim_data_future_rescaled/",
                                 scenarios,
                                 ".rds"))
```


c) Seting up some important variables:

```{r}
# Cell hight and width for the grid.
# This value depends on rasters projection. If rasters are in UTM, values will be in meters. If rasters are in decimal degrees (as in WorldClim 2.1), values will be in degrees. However, note that the function make_grid (used to build the grid above study area) has an argument called epsg where we can reproject spatial data. The epsg of study area is further transmitted to predictor variables. This means that even if WorldClim 2.1 is projected in decimal degrees we should address cell sizes in the desired epsg.

# Following values build a grid with 100 x 100 km.
epsg = 6933
cell_width = 100000
cell_height = 100000
# Note that setting those values to build a grid smaller than the input rasters may generate NaNs, causing some problems.

# If you have any variable in shape_study_area that you want to keep for rescaling, you can set here.
# Set the correct names of variables using as much as 10 characters.
# Setting the names to list() will use none of the variables in the shape, while setting it to NULL will use all variables.
names_var_shp_study_area <-  c("LENGTH_KM", "DIST_DN_KM", "DIST_UP_KM", "CATCH_SKM", "UPLAND_SKM", "DIS_AV_CMS", "ORD_STRA")
raster_vars <- paste0('bio_', 1:19)

# As in the codeline above, here we set which variables in current rasters we want to keep for rescaling.
# Set the correct names of variables using as much as 10 characters.
# Setting the names to list() will use none of the variables in the shape, while setting it to NULL will use all variables.
current_var_names <- c(names_var_shp_study_area, raster_vars) # or NULL

# As in the codelines above, here we set which variables in future rasters we want to keep for rescaling.
# We will usually need at least the same variables as in the current scenario for projection.
# Set the correct names of variables using as much as 10 characters.
# Setting the names to list() will use none of the variables in the shape, while setting it to NULL will use all variables.
future_var_names <-  current_var_names 
```


## Study Area
The map of study area needs to be imported to R, so we can create a grid for the study area. This grid will be used for model building and projections.

```{r study area}
shape_study_area <- shape_study_area_file %>%
  st_read() %>%
  repair_shp()

if (output_shp_study_area %>% file_exists()== F){
  grid_study_area <- shape_study_area %>% 
      make_grid(cell_width, cell_height, names_var_shp_study_area, epsg=epsg, centroid = F) # target EPSG
  
  output_shp_study_area %>% 
    path_dir() %>% 
    dir_create()
  
  grid_study_area %>% st_write(
      dsn = output_shp_study_area)
} else {
  grid_study_area <- output_shp_study_area %>% st_read()
}
```


## Rescaling variables
The next step aims to cross data from study area with rasters of variables.
We will start with current data.

```{r rescaling current}
### Rescaling current data
## Error in attempt to apply non-function
if (output_shp_current %>% file_exists() == F) {
  grid_current <- grid_study_area %>% 
    add_raster(folder_current_rasters, raster_vars) 
    
  output_shp_current %>% 
    path_dir() %>% 
    dir_create()
  
  grid_current %>% saveRDS(output_shp_current)
  grid_current %>% as.data.frame %>% select(!'geometry') %>% 
    write.csv(here("output_data/WorldClim_data_current_rescaled/Colossoma_current.csv"))
} else {
  grid_current <- output_shp_current %>% readRDS()
}
```


Now within a loop to rescale future variables.

```{r rescaling future}
### Rescaling future data
if (!all(output_shp_future %>% file_exists())) {
  for (i in 1:length(scenarios)) {

      grid_future <- grid_study_area %>% 
       add_raster(folder_future_rasters, future_var_names, scenarios[i]) 
      
      output_shp_future %>% 
       path_dir() %>% 
       dir_create()
      
      grid_future %>% saveRDS(output_shp_future[grep(scenarios[i], output_shp_future)])
      l2 <- gsub('.rds$', '.csv', output_shp_future[grep(scenarios[i], output_shp_future)])
      grid_future %>% as.data.frame %>% select(!'geometry') %>% write.csv(l2)  
  }
}

grid_future <- lapply(output_shp_future,function(x){readRDS(x)})
names(grid_future) <- scenarios

print(grid_future[[1]])
```


# Occurrence data
## Open files with data
It is necessary to name the output. Be extra careful with extension names.
a) Input:

```{r input_own_data, eval=T, include=T}
spp_data <- here('input_data/spp_data.csv')
```

b) Output:

```{r files output occurrence, eval=T, include=T}
#  Set the path to the output shapefile, which will contain the presence/absence matrix.
spp_output <- here("output_data/spp_pa.shp")
```


It is also necessary to set some other important parameters.

```{r , eval=T, include=T}
# Species names to be used in the study. 
# Names should be identical from input/spp_data.csv obtained previously.
# Setting this to NULL will use all species.
#spp_names <- especie # or NULL
spp_names <- c("Colossoma.macropomum")
```


## Importing occurrence data

```{r importing occurrence data,  eval=T, include=T}
occ_shp <- spp_data %>% 
  occurrences_to_shapefile(spp_names, grid_study_area)


mapview(grid_study_area[,1], alpha.regions = 0, color = "red", lwd = 1, layer.name = "Study Area", legend=NULL) +
  mapview(occ_shp, zcol = "species", layer.name = "Species") 
```


## Occurrence grid for the Study Area
We will say to the grid_study_area which cells have an occurrence record for the studied species.

```{r generating_shape_matrix_pa, eval=T, include=T}
  spp_names_abv <- spp_names %>% 
    to_snake_case() %>% 
    abbreviate(minlength = 10) %>% 
    as.vector()

if (spp_output %>% file_exists() == F) {
  grid_matrix_pa <- occ_shp %>% 
    occurrences_to_pa_shapefile(grid_study_area, spp_names)
  
  spp_output %>% 
    path_dir() %>% 
    dir_create()
  
  grid_matrix_pa %<>% select(all_of(spp_names_abv))
  
  grid_matrix_pa %>% st_write(dsn = spp_output)
} else {
  grid_matrix_pa <- spp_output %>% st_read()
}

grid_matrix_pa %>% 
  as.data.frame() %>% 
  select(all_of(spp_names_abv)) %>% 
  rowSums() %>% 
  as.vector() %>% 
  richness_map(., grid_study_area)
```


Check how many records there is to each species:

```{r number of presences, eval=T, include=T}
presences_number(grid_matrix_pa, spp_names)
```


# Variable Selection - VIF
Variable Selection must be done OR with a VIF routine, OR with a PCA.
To perform a VIF routine, we will use only the environmental data from the occurrence data.

```{r vif_bio, eval=T, include=T}
# Obtain occurrence data.frame and environmental variables:
df_vif <- get_df_vif(grid_current, grid_matrix_pa)

# To pre-select variables use the function select:
# df_vif <- df_vif %>% select("length_km", "bio_1", "bio_2", "bio_3", "bio_4", "bio_12", "bio_13", "bio_14")

# Or you can delete undesired variables using -c() in select function as well:
# df_vif <- df_vif %>% select(-c("length_km", "bio_1", "bio_2", "bio_3", "bio_4", "bio_12", "bio_13", "bio_14"))

# Run VIF routine from usdm package:
vif_bio <- lapply(df_vif, function(x){vifcor(x,th=0.5)})
vif_bio[[1]]

# We can exclude variables with high VIF:
grid_current <- sapply(names(df_vif), function(x){select(grid_current, !vif_bio[[x]]@excluded)}, USE.NAMES = T, simplify = F)
                        
```

```{r setting up objects, eval=T, include=T}
shp_matrix_pa <- grid_matrix_pa

df_species <- shp_matrix_pa %>% 
  as.data.frame() %>%
  select(-c('geometry'))

df_var_preditors <- output_shp_current %>%
  get_predictors_as_df()

df_potential_predictors <- df_species %>%
  bind_cols(df_var_preditors)

df_potential_predictors %>% 
  head() %>% 
  round(4) %>% 
  datatable(options = list(pageLength = 10, scrollX=T))
```

# Training Models
## Set Data
As in previous steps, it is necessary to set inputs and outputs, taking extra care with extension names.
a) Input:
```{r , eval=T, include=T}
# To use raw predictors (VIF):
shp_matrix_pa <- grid_matrix_pa
df_var_preditors <- lapply(grid_current, function(x){as.data.frame(x) %>% select(-c('geometry', 'cell_id'))})
grid_future <- lapply(output_shp_future,function(x){readRDS(x)})
names(grid_future) <- scenarios
df_var_preditors[[1]]
```


b) Outputs:

```{r , eval=T, include=T}
# Name the directory to save trained models.
folder_models <- here("output_data/models")
```


c) Control Variables:

```{r control_variables_training, eval=T, include=T}
# Algorithm names to be used in Species Distribution Modeling.
# Run getmethodNames() to unveil which algorithms are available.
algo <- c("rbf", "svm", "mda")

# Set the threshold criteria.
# 1:sp=se, 2:max(se+sp), 3:min(cost), 4:minROCdist, 5:max(kappa), 6:max(ppv+npv), 7:ppv=npv, 8:max(NMI), 9:max(ccr), 10: prevalence
thresh_criteria <- 2

# Number of runs to each algorithm
n_run <- 10

# Number of folds to crossvalidation
n_folds <- 4

# Number of pseudoabsence sets
n_pa <- 1
```


## Generate Pseudoabsences
To build models, it is necessary to use pseudoabsences that contrast to presences. Currently, only the 'random' and 'envelope' methods are applied.

```{r generate pseudabsences,  eval=T, include=T}
df_pseudoabsences <- shp_matrix_pa %>%
  pseudoabsences(df_var_preditors, spp_names, method="envelope", folder_models) 
```


It is possible to plot a t-SNE graph to check whether pseudoabsence data clusters into a separate group from presence data.

```{r tsne plot,  eval=T, include=T}
tsne_list <- df_potential_predictors %>% 
  tsne_plot(df_pseudoabsences, spp_names)
tsne_list
```


## Join data
As we are using the sdm package, let's start to build our models by indicating our input data.
```{r vif_pseudoabsence,  eval=F, include=T}
# For VIF routine:
for(s in colnames(df_species)){
  df_pseudoabsences[[s]] <- exclude(df_pseudoabsences[[s]], vif_bio[[s]])
}
```

```{r fitting data, eval=T, include=T}
d <- df_species %>% 
  fit_data(df_var_preditors, df_pseudoabsences)

d[[1]]
```


## Training Models
With the data, we can build our models.

```{r training, eval=T, include=T}
df_species %>%
  train_models_to_folder(
      d, 
      algo, 
      n_run, 
      n_folds, 
      folder_models
    )

folder_models %>%
  dir_ls() %>%
  path_file() %>% 
  head() %>%
  as.data.frame()

"Number of trained species: " %>%
  paste0( 
    folder_models %>%
      dir_ls() %>% 
      length()
  ) %>%
  print()
```


We can check for the response curves:

```{r response_curves, eval=T, include=T}
m <- sp_model_from_folder(colnames(df_species), folder_models)
rcurve(m$clssm_mcrp) # all models
id_mda <- getModelId(m$clssm_mcrp)[getModelInfo(m$clssm_mcrp)$method == 'mda']
id_svm <- getModelId(m$clssm_mcrp)[getModelInfo(m$clssm_mcrp)$method == 'svm']
id_rbf <- getModelId(m$clssm_mcrp)[getModelInfo(m$clssm_mcrp)$method == 'rbf']
rcurve(m$clssm_mcrp, id=id_mda) # only mda models
rcurve(m$clssm_mcrp, id=id_svm) # only svm models
rcurve(m$clssm_mcrp, id=id_rbf) # only rbf models
```


How many models failed?

```{r fails, eval=F, include=T}
d %>% 
  model_failures(folder_models)
```


## Model Selection and Threshold visualization

```{r thresholds, eval=T, include=T}
spp_names <- colnames(df_species)

thresholds_models <- spp_names %>% 
  sp_thresh_from_folder(folder_models, thresh_criteria)

thresholds_models_means <- spp_names %>%  
  validation_metrics_from_folder(folder_models, thresholds_models)

model_selection <- spp_names %>%  
  validation_metrics_from_folder(folder_models, thresholds_models, stats = 'AUC', th = 0.8) # mudar o th pra 'mean' ou 'best' ou integer com número de melhores? incluir mais opções de stats=c('AUC', 'TSS')

thresholds_models_means[[1]]
```


To see the mean AUC values:

```{r auc_values, eval=T, include=T}
spp_names %>%  
  validation_metrics_from_folder(folder_models, thresholds_models, stats = 'AUC')
```

# Projections
To project our models in space, we need to set where the models were saved (previously set as the folder_models object) and where we want to save our projections.

```{r output directory projections, eval=T, include=T}
directory_projections <- here("output_data/projections")
```


Set up some variables.

```{r prepare_predictions,  eval=T, include=T}
df_pa <- shp_matrix_pa %>% 
  as.data.frame() %>%
  select(-c('geometry'))

df_potential_predictors <- df_pa %>% 
  bind_cols(df_var_preditors)

projection_data <- lapply(grid_future, function(x){ x <- as.data.frame(x)
                                           x[!(names(x) %in% c("x_centroid", "y_centroid", "geometry"))]})
projection_data$current <- df_var_preditors
```


And finally run our projections.

```{r pred_distribution, eval=T, include=T, warning=FALSE}
# Project models in scenarios
df_pa %>% predict_to_folder(scenarios_list=projection_data,
                              models_folder=folder_models, 
                              pred_methods=model_selection, 
                              thr_criteria=thresh_criteria, 
                              output_folder=directory_projections,
                              thresholds_models_means=thresholds_models_means)
```
  
  
# Visualizing Results
## Obtain predictions

```{r map_pa, eval=T, include=T}
predictions_sp <- sapply(spp_names, function(x){sp_predictions_from_folder(x,directory_projections)},simplify=F, USE.NAMES = T)
pred_means <- predictions_means(predictions_sp, c(scenarios, 'current'))
ensembles <- gcm_ensemble(pred_means, ssp=c('current', 'ssp245'))

# Output ensembles
for (i in 1:length(ensembles)) {
  write.csv(ensembles[[i]], paste0(directory_projections,'/',names(ensembles)[i],'.csv'))
}
```


## Frequence map in current scenario

```{r map_frequency_ensemble, eval=T, include=T}
ensemble_map(ensembles$current$current_freq_mean, grid_study_area, "Current", 'Frequence')
```


## Presence map in current scenario

```{r richness_map1, eval=T, include=T}
ensemble_map(ensembles$current$current_pa_mean, grid_study_area, "Current", 'Presence')
```


## Frequence map in SSP2-4.5/2090

```{r richness_map2, eval=T, include=T}
ensemble_map(ensembles$ssp245$freq_mean, grid_study_area, "SSP2-4.5/2090", 'Frequence')
```


## Presence map in SSP2-4.5/2090

```{r richness_map3, eval=T, include=T}
ensemble_map(ensembles$ssp245$pa_sums, grid_study_area, "SSP2-4.5/2090", 'Presence')
```
